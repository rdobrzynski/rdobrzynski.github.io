
---
title: "Midterm Project--Ryan Dobrzynski"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
Data Import
```{r}
library(tidyverse)
df<-read_csv("midterm_data.csv")

```
---

class: inverse, middle, center
# Visualization Plots

---
```{r}
df %>% ggplot()+
  geom_point(mapping = aes(x = df$los, y=df$tot))+labs(x="Length of Stay",y="Total Bill",title="Length of Stay vs Total Bill",caption="As length of stay increases, the total bill for the patient tends to increase.")
```

---
```{r}
df %>% ggplot()+
  geom_bar(mapping = aes(x = df$sex))+scale_x_continuous(limits = c(0,3))+labs(x="Sex: 1=Male, 2=Female",title="Total Patients for each Sex",caption="Female has about 10,000 more patients than males.")
```

---
```{r}
df %>% ggplot()+geom_bar(mapping=aes(x=df$moa))+labs(x="Months of Admission",title="Total Patients for each Month",caption="The months are fairly even for admissions.")
```

---
```{r}
df %>% ggplot()+geom_col(mapping=aes(x=df$sex,y=df$los))+scale_x_continuous(limits = c(0,3))+labs(x="Sex: 1=Male, 2=Female",y="Length of Stay",title="Length of Stay for each Sex",caption="Female stay slighlty longer than males.")
```

---
```{r}
df %>% ggplot()+geom_point(mapping=aes(x=df$age,y=df$los))+labs(x="Age",y="Length of Stay",title="Age vs Length of Stay",caption="Age does not seem to be a factor in determining length of stay.")
```

---
```{r}
df %>% ggplot()+geom_point(mapping=aes(x=df$age,y=df$los))+facet_wrap(~df$sex)+labs(x="Age",y="Length of Stay",title="Age vs Length of Stay",caption="1 is male and 2 is female, it appears there are 4 cases labeled as 9 as well.")
```

---
```{r}
df %>% ggplot()+geom_bar(mapping=aes(x=df$provider))+labs(x="Provider Number",title="Providers Total Patients",caption="Provider 7205 has the most patients.")
```

---
```{r}
df %>% ggplot()+geom_point(mapping=aes(x=df$age,y=df$tot))+facet_wrap(~df$raceethn)+labs(x="Age",y="Total Charged",title="Age vs Total Charged by Races",caption="For most of the races, it seems like youngest patients have the highest charge.")
```

---
```{r}
df %>% filter(pt_state %in% c("RI",'MA')) %>% ggplot()+geom_point(mapping=aes(x=age,y=tot))+facet_wrap(~pt_state)+labs(x="Age",y="Total Charged",title="Age vs Total Charged by State",caption="For both states, it seems like youngest patients have the highest charge.")
```

---
```{r}
library(lubridate)
df %>%filter(pt_state %in% c("RI",'MA')) %>% ggplot()+geom_bar(mapping=aes(x=pt_state))+labs(x="State",caption="There are far more patients from RI than MA.",title="Patients in RI and MA")
```

---
```{r,echo=FALSE}
library(gganimate)
df$months<-month.abb[df$moa]
df %>% filter(pt_state %in% c("RI",'MA')) %>% ggplot(mapping=aes(x=moa,y=tot,color=pt_state))+geom_line()+
  transition_reveal(moa)+labs(x="Month of Admission",y="Total Bills",color="State",title="Total Bills throughout the year")
```
---
class: inverse, middle, center
# Predictive Models
---
```{r,message=FALSE, warning=FALSE}
df$target<- ifelse(df$tot<=21854,"low","high")
d <- df[,c("age","sex","raceethn","provider","moa","mod","admtype","campus", 'los','target')]
d <- d %>% filter(sex!=9)
d <- d %>% filter(raceethn!=' ') %>% filter(admtype!=' ')
d$raceethn <- factor(d$raceethn)
d$provider <- factor(d$provider)
d$admtype <- factor(d$admtype)
d$campus <- factor(d$campus)
d$sex <- factor(d$sex)
library(caret)
splitIndex <- createDataPartition(d$target, p = .05,                           list = FALSE)
train <- d[ splitIndex,]
test <- d[-splitIndex,]
```
---
```{r,message=FALSE, warning=FALSE, echo=FALSE}
trControl = trainControl(method = "cv",
                         number = 5)

forest_ranger <- train(target~., data=train, 
                    method = "ranger", 
                    trControl = trControl,
                    na.action=na.omit)

tree_approach <- train(target~., data=train, 
                                method = "rpart2", 
                                trControl = trControl,
                       na.action=na.omit)
glm <- train(target~., data=train, 
                                method = "glm", 
                                trControl = trControl,  na.action=na.omit)
```                               
---                       
```{r,message=FALSE, warning=FALSE,echo=FALSE}
results <- resamples(list('Random Forest' = forest_ranger,
                          'Decision Tree' = tree_approach,
                          'Generalized Linear Model'= glm))
bwplot(results)
```
---
```{r}
pred <- predict(forest_ranger, test)
table(pred)
cm <- confusionMatrix(data = pred, reference = as.factor(test$target), positive = "high")
cm$overall[1]
```

The best model appears to be the Random Forest. This model was used to predict the target variable in the test data set. The accuracy of the model was .82. 
---
A new Target variable was made that classifies whether a patient's stay was short or long based on the average length of stay. 
```{r}
df$Targ<- ifelse(df$los < 5,"short","long")
df$Targ <- factor(df$Targ)
ds <- df[,c("age","sex","raceethn","provider","moa","mod","admtype","campus", 'tot','Targ')]
ds <- ds %>% filter(sex!=9) %>% filter(raceethn!=' ') %>% filter(admtype!=' ')
```
---
Data cleaned and split into training and test. 
```{r}
ds$provider <- factor(ds$provider)
ds$admtype <- factor(ds$admtype)
ds$campus <- factor(ds$campus)
ds$sex <- factor(ds$sex)

splitIndex1 <- createDataPartition(ds$Targ, p =.05,                           list = FALSE)
train1 <- ds[ splitIndex1,]
test1 <- ds[-splitIndex1,]
trControl1 = trainControl(method = "cv",
                         number = 5)
```
---
Three models were made to test which is the most accurate. 
```{r,message=FALSE, warning=FALSE, echo=FALSE}
forest_ranger1 <- train(Targ~., data=train1, 
                    method = "ranger", 
                    trControl = trControl,
                    na.action=na.omit)

tree_approach1 <- train(Targ~., data=train1, 
                                method = "rpart2", 
                                trControl = trControl,
                       na.action=na.omit)
glm1 <- train(Targ~., data=train1, 
                                method = "glm", 
                                trControl = trControl,  na.action=na.omit)

results1 <- resamples(list('Random Forest' = forest_ranger1,
                          'Decision Tree' = tree_approach1,
                          'Generalized Linear Model'= glm1))
bwplot(results1)
```
---
```{r}
pred1 <- predict(forest_ranger1, test1)

cm1 <- confusionMatrix(data = pred1, reference = test1$Targ, positive = "short")

cm1$overall[1]
```

The best model appears to be the Random Forest. This model was used to predict the target variable in the test data set. The accuracy of the model was .84.
